AI-Powered Home Care Bundle Builder – Next-Generation Implementation Plan

Overview and Objectives

The AI-Powered Home Care Bundle Builder is a system that generates personalized home care service “bundles” for high-needs patients, based on clinical assessments (interRAI data, CAP triggers, RUG-III groupings, etc.) and patient/caregiver goals. The previous implementation successfully developed core logic (assessment parsing, service intensity matrix, scenario generation) and even integrated an AI-based explanation feature using Vertex AI. However, we are now undertaking a major rebuild of the system to improve its functionality, maintainability, and integrate it with new AI tooling. The key objectives of this next-gen build are:
	•	Full Reimplementation on a New Stack: Rebuild all core components on a unified, modern codebase, avoiding any “Frankenstein” mix of old and new code (per our decision to use Option C for integration). This ensures a clean architecture without compatibility hacks or legacy baggage.
	•	Leverage Advanced AI Models in Development and Runtime: Utilize Anthropic’s Claude 4.5 (Opus) as the primary coding and reasoning model, taking advantage of its large context window and coding capabilities . Supplement this with other models (OpenAI GPT-4 via ChatGPT, Google’s Gemini, etc.) for cross-validation, creativity, or specialized subtasks as needed.
	•	Maintain and Improve Domain Logic: Preserve the validated logic from the current system (e.g. Clinical Assessment Protocol (CAP) triggers, service intensity rules, scenario “personas”)  , but implement them in a more modular, data-driven, and extensible manner. Where possible, improve upon the original (Option A with enhancements) rather than copying code verbatim – for example, refining algorithms, removing workarounds, and optimizing performance.
	•	Ensure Contextual Integrity and PII Safety: Continue to enforce privacy (e.g. PII masking in prompts as previously done) and ensure that any patient data processed by AI (for explanations or otherwise) remains secure and anonymized.
	•	User Experience Parity (or Better): The new system should provide all the features of the old one (e.g. multiple scenario bundles with explanations, UI indicators for CAP triggers, etc.) and ideally introduce improvements. We will verify that the new build meets or exceeds the capabilities of the previous phases (Phases 1–7 as per the feature list)  .

By rebuilding from scratch with these goals, we aim for a robust, cohesive application where all parts work seamlessly together on the new basis (no piecemeal integration issues). Below we detail the design and implementation plan, including tooling strategy, development workflow, and component-by-component approach.

Technology Stack and Tools

Primary AI Model – Claude 4.5 (Opus): We will use Claude 4.5 Opus as the chief development assistant and possibly also integrate it into the application for on-demand reasoning (e.g. generating bundle explanations). Claude 4.5 offers a very large context window (up to ~200k tokens) , which means it can ingest extensive portions of our specification or codebase when generating outputs. It’s also noted as highly capable in coding tasks and complex reasoning, making it ideal for both writing our code and powering any AI-driven features within our app (like explanatory text). We have access to the Claude 4.5 Max plan, which ensures we can use this model at high capacity (20× the standard usage limits).

Supporting Models: In addition to Claude, we have access to OpenAI’s GPT-4 (via ChatGPT Pro) and Google’s Gemini (through our Google One subscription). We will leverage these models in a complementary way:
	•	ChatGPT (GPT-4) – for double-checking code or logic generated by Claude. GPT-4 is very good at catching subtle errors or suggesting alternative approaches. We might, for instance, have GPT-4 review a function or test cases that Claude wrote to identify edge cases or bugs.
	•	Google Gemini – for specialized tasks like summarizing large documents or offering a different perspective on a problem. Gemini (especially if accessible via Bard or Vertex AI) might have strengths in certain knowledge domains or creativity. For example, if we need to summarize an interRAI assessment form or clinical guideline to include in a prompt, we could use Gemini to condense that info. We can also use it to verify that our prompts or outputs are not missing any context from a healthcare perspective, since Google’s models might have been trained on extensive medical data.

Cursor IDE and Agents: Development will take place in the Cursor IDE, which provides AI agent integrations within a coding environment. We have the Cursor Ultra plan, enabling advanced features like Parallel Agents and use of custom models. Cursor’s agent acts like an AI pair-programmer that understands the codebase and can make changes or create new files based on natural language instructions . Key capabilities we’ll utilize:
	•	Parallel Agents: We can run multiple AI agents concurrently on different tasks . Each agent operates in an isolated git worktree, meaning they work on separate branches of the repo without conflict . This is useful for a large project – for instance, one agent can work on the frontend UI while another builds the backend API simultaneously. We plan to use Option B (parallel development) to speed up the build where feasible, instead of a strictly sequential single-agent approach. This requires carefully partitioning tasks so they don’t overlap on the same files.
	•	Multiple Model Access: Cursor allows using different models in different agents or for different prompts. For example, we might configure one agent to use Claude (for major coding tasks) and another to use GPT-4 or Cursor’s own model, and then compare outputs (a Best-of-N approach) . This way, if one model misses something, another model’s output might fill the gap. We will take advantage of this to mitigate errors – running the same prompt on two models in parallel and choosing the better result, when appropriate.
	•	Terminal and Execution Integration: Cursor agents can run shell commands and tests. We will have the agent run our code’s unit tests or sample scenarios to verify that each component works as expected, catching issues early. This helps navigate one weakness of AI-generated code: it may compile/run incorrectly on first try. By letting the agent execute tests in the environment, we can prompt it to fix any failing test.

In summary, our stack for this rebuild is Claude 4.5 + Cursor as the core development duo, augmented by GPT-4 and Gemini for quality assurance and specific subtasks. The runtime application itself will likely be built in a language well-supported by these tools (e.g. Python or TypeScript, or potentially remain in Java if that was the old stack, but likely we’ll choose a language where AI assistance is strongest and which suits quick iteration – Python is a strong candidate unless performance dictates otherwise). All development will ensure the code remains accessible for human developers too (clear structure, comments, and documentation) since maintainability is a priority.

Prompting Strategy and Context Window Management

One of the challenges in using AI for a complex project is managing the context window – ensuring the model has all the information it needs without exceeding what it can handle in a single prompt. Claude 4.5’s context window is extremely large (~200k tokens) , which in theory could fit our entire specification and even codebase. However, practical limits (like prompt construction complexity, potential degradation of response quality with extremely long prompts, and performance considerations) mean we should be strategic in how we present context to the model. We prefer Option A (monolithic context) where possible – i.e. giving Claude a comprehensive specification in one go – because it ensures the model sees the full picture and can make globally consistent decisions. But if the content becomes too long or unwieldy for effective prompting (or if we notice the model struggling or timing out), we will fall back to Option B: a modular, chunked approach.

Our prompting strategy will include the following guidelines:
	•	Structured Specification: We will feed instructions and information to the model in a structured Markdown format (like this document). The spec uses clear headings, bullet lists, and short paragraphs (as per the user’s formatting guidelines) to make it easy for the model to parse. Important details (e.g. data fields, rules, example inputs/outputs) will be itemized. This structured approach helps the model focus on relevant sections when generating code for a specific component.
	•	Include Essential Context, Prune the Rest: We will include domain knowledge that’s essential for the task at hand in the prompt, but avoid overwhelming the model with unnecessary text. For example, when coding the Service Intensity Matrix logic, we will provide the relevant CAP definitions or triggers mapping (possibly summarized) but not, say, the entire text of all interRAI forms. We might summarize key points from the interRAI CAPS documentation (e.g. what triggers the Falls CAP and what interventions it suggests  ) rather than raw text. If needed, we can use a model like Gemini to pre-summarize long clinical guidelines into short bullets that Claude can digest easily.
	•	Utilize the Large Context Wisely: For major generation tasks – such as writing the whole Scenario Generator module – we will attempt to include all necessary interfaces and data structures in the prompt so Claude can refer to them. For instance, if the ScenarioGenerator code needs to interact with a PatientNeedsProfile class and a ServiceCatalog of services, the prompt to generate ScenarioGenerator should include the definitions (or at least the method signatures and relevant fields) of those classes. Claude’s context size allows us to provide these, ensuring the model writes code that aligns with the rest of the system.
	•	Chunk by Functional Units: If a single component is extremely large, we will break the prompt into sub-tasks. For example, rather than one giant prompt “Build the entire backend”, we might do: “Build the data model classes” as one prompt, “Build the CAP trigger engine” as another, “Build the scenario generation logic” as another, etc. Each prompt will include the spec section relevant to that component plus any cross-cutting info needed. This modular approach (Option B) keeps each prompt focused and within manageable length. We have to be mindful that the entire conversation history counts towards context, so doing too many sequential chunks could eventually overflow the window if we keep everything. We will therefore periodically summarize or reset context after finishing a module, then load the summary of that module (or its interface) into the next prompt for a new module.
	•	Check Context Usage in Real Time: We will monitor the token length of prompts. If we approach, say, 150k tokens (which is extremely high), we might opt to not add more. Claude 4.5 can handle up to 200k, but leaving some headroom is wise so it has space to generate a thorough answer. We also have the effort parameter available in Claude  – for very large prompts, we might set a high effort to encourage Claude to use more tokens in the response (ensuring it doesn’t cut off critical content). However, high effort also consumes more tokens, so it’s a balance.

In summary, we will be mindful of context limits at all times. Our approach is to give the model as much information as needed for accuracy (preferring completeness), but if the attempt to include “everything” makes the prompt unwieldy, we will intelligently partition the work. This dual strategy ensures we neither overwhelm the model nor starve it of needed context.

AI-Driven Development Workflow

To efficiently build this complex system, we will adopt an AI-driven workflow with parallelization and verification steps, aligning with Option B from our planning (using multiple agents and multi-step refinement). The development process will look like this:
	1.	Task Breakdown: We will divide the project into clear tasks or components. For example: Data Model Definition, Assessment Ingestion Service, Service Intensity Matrix, CAP Trigger Engine, Scenario Generator, Explanation Service, Frontend UI, etc. Each task will be relatively self-contained. We’ll order tasks so that dependencies are respected (e.g., define data models before scenario logic that uses them), but some can be done in parallel (e.g., UI can be started alongside backend development using agreed API contracts).
	2.	Parallel Agent Assignment: Where tasks are independent, we will run parallel Cursor agents. For instance, Agent A (with Claude) might tackle the backend logic while Agent B (with Claude or GPT-4) works on the frontend. Or Agent A focuses on the CAP trigger module while Agent B works on the scenario generation module. Thanks to Cursor’s worktree isolation, each agent will have its own branch . We must ensure each agent’s context includes any interfaces or contracts needed to avoid divergent assumptions. For example, if both modules need the definition of a ServiceBundle class, we provide that definition to both agents in their prompts so they stay consistent.
	3.	Multi-Model Cross-Verification: For critical pieces of code, we will employ a multi-model strategy. This could mean:
	•	Running the same prompt on Claude and GPT-4 in parallel (two agents) and then comparing results. We may choose the better one or even merge ideas from both. This best-of-both-worlds approach leverages each model’s strengths .
	•	Alternatively, after Claude produces an output, we can prompt ChatGPT (manually or via another agent) to review the code for issues. For example, “Here is a function generated by another AI. Do you see any bugs or improvements?” GPT-4 might catch logic errors or suggest edge cases that Claude’s output missed.
	•	We can also let a model like Gemini provide a sanity check for domain-specific accuracy (e.g., “Does the explanation text sound clinically accurate and clear?”).
	4.	Incremental Development and Testing: After an agent finishes a task (e.g., generates the code for the CAP Trigger Engine), we will compile/run that module and execute tests. Cursor agents can run such tests in the integrated terminal. If a test fails or runtime error occurs, we can feed the error back to the agent to have it debug and fix the code. This iterative loop continues until the module passes its tests. Testing each unit immediately mitigates the risk of compounding errors – a known weakness of AI-generated code is that mistakes can propagate if not caught early. By validating step by step, we ensure each part works before integrating.
	5.	Integration of Components: Once modules are individually built and tested, we will integrate them. This might involve merging the git branches from parallel agents. We should do a careful merge review. Because each agent worked in isolation, there could be minor mismatches (e.g., one agent’s function name doesn’t exactly match what another expected). We will resolve these manually or with a quick AI prompt to adjust names/types for consistency. After integration, we will run end-to-end tests (for example: input a sample assessment all the way through to bundle scenarios output) to verify the whole flow works (as was done in the original Phase 4 integration test ).
	6.	Refinement Passes: With the system integrated, we’ll do additional passes using AI to refine and document. For instance, ask Claude to generate docstrings or comments for the functions if they are lacking, or to improve code style for readability. We will also generate user-facing documentation (like a README or even an internal manual) describing how the bundle builder works, using the AI to draft it and then editing as needed. This ensures not only the code but also the documentation is comprehensive.
	7.	Human Oversight at Each Stage: Despite heavy use of AI, we will maintain human-in-the-loop oversight. This means a developer (us) will review AI outputs, run tests, and guide the prompts. Past experience and external reports show that AI coding assistants can produce incorrect or suboptimal code for complex projects if left unchecked . We anticipate needing to correct course occasionally. Having multiple models and tests as described helps, but ultimately human judgment will ensure the final product meets quality standards.

By following this workflow, we harness the speed of AI generation while counteracting its weaknesses:
	•	Parallel agents and multiple models give us breadth and alternative solutions rather than relying on a single output.
	•	Automated testing and iterative debugging give the AI feedback, improving reliability.
	•	Structured task breakdown prevents the AI from getting “confused” by too much complexity at once.
	•	Human oversight catches any conceptual mistakes that automated tests might miss (or where AI confidently asserts something wrong).

This approach aligns with modern best practices for AI-assisted development, essentially pair-programming with the AI and using it as a force multiplier while not relinquishing all control.

Redevelopment of Core Components (Improved Designs)

We will now detail how each major component or module of the Home Care Bundle Builder will be (re)implemented in this new build, highlighting improvements over the previous version. The guiding philosophy is Option A with improvements – i.e., design each component afresh for this new system, using the knowledge gained from the prior implementation but not necessarily copying its code verbatim. This allows us to address any limitations or tech debt from before and tailor the components to work optimally with our new architecture.

1. Data Models and Configuration

PatientNeedsProfile & Related Data Structures: We will create classes or data structures representing the patient’s assessment data (e.g., PatientNeedsProfile, which includes domains like Functional Status, Cognition, Clinical Risks, Caregiver context, etc.). In the old system this profile aggregated interRAI assessment outputs into key algorithm inputs  . In our new build, we’ll ensure this profile is well-defined (likely as a Python dataclass or similar) with clear fields for each relevant metric (ADL scores, CPS, CHESS, caregiver distress, etc.). An improvement here is to make the profile extensible – if new assessment sections (like the BMHS mental health screener) are added, we can easily extend the profile. Indeed, the previous project had a Phase 6 to integrate BMHS, adding new fields; our design will accommodate such additions from the start (perhaps by not hard-coding field lists in too many places, or using a flexible data schema).

Service Catalog: The system has a catalog of possible services (e.g., Personal Support hours, Nursing visits, Physiotherapy, meal delivery, respite care, technology like Lifeline alarms, etc.). We will represent this as a configuration (maybe a JSON or YAML file, or a Python dictionary) that lists all services and their attributes (like cost per hour, maximum allowed hours per week, etc.). This was partly encoded in the prior system’s logic (possibly in code or config files). We will explicitly separate it as data, so that updating service definitions doesn’t require code changes. This is an improvement for maintainability.

Clinical Logic Configuration (CAPs and Algorithms): The rules linking clinical data to services (e.g., if Falls CAP is triggered, include physiotherapy and OT with certain frequencies ) will be encoded in a rules config (possibly a set of YAML files, one per CAP or per algorithm). The previous implementation did something similar, as evidenced by files like falls.yaml, pain.yaml, etc. listed in the feature list . We will reuse those definitions, but verify them for correctness and possibly update them to new evidence if available. Also, we’ll incorporate additional CAPs that might not have been included yet (the note mentions bowel, continence CAP could be added ). In our improved version, adding a new CAP trigger should be as simple as creating a new YAML file and maybe a small extension to the code to load it – no deep code changes. This data-driven approach increases flexibility.

Data Storage and Access: For the scope of building the prototype, we might use in-memory data or simple files. But we should design with future production in mind – likely there will be a database (if this were a real deployed system) for storing patient assessments and bundles. For now, our focus is on the logic, but we will structure the code such that integrating a database later is not hard (e.g., abstract the data access layer). Option B was chosen earlier for using a new basis for everything, which includes data storage. That means we will not attempt to directly connect to any old database schema if one existed; instead, we define our own schema/tables as needed, and later data migration can be handled if required. Not mixing old with new ensures we aren’t constrained by legacy database design that might not fit our improved logic.

2. Assessment Ingestion and Scoring Engine

This module takes raw assessment inputs (likely the interRAI assessment form data, e.g., a set of responses or scores from the interRAI Contact Assessment or HC assessment) and computes derived scales and triggers. In the prior system, an AssessmentIngestionService was responsible for computing algorithm scores and preparing the PatientNeedsProfile . We will implement a similar service.

Functions:
	•	Parse the raw assessment data (maybe coming in JSON form or from a CSV) into our PatientNeedsProfile.
	•	Calculate important summary scores (ADL Hierarchy, IADL difficulty, CPS (Cognitive Performance Scale), CHESS, Pain scale, Depression Rating, etc.) using established algorithms.
	•	Determine which CAPs are triggered at what level (improve, prevent, manage, etc.) .
	•	Map any supplemental assessments (like BMHS for mental health risk) into the profile.

Improvements: We will ensure this ingestion is modular. If tomorrow a new assessment type is introduced, we can add a mapper for it. For example, the BMHS integration was a separate phase before; in our design, adding BMHS would just be plugging in an additional mapper without affecting core logic. We can achieve this by designing the service to load multiple mapper components (one for interRAI HC, one for BMHS, one for others). Another improvement is to incorporate validation – if assessment data is inconsistent or incomplete, the service should flag that (e.g., throw an exception or return an error). This helps with data quality, ensuring our subsequent logic isn’t garbage-in.

3. Service Intensity Matrix and CAP Trigger Engine

This is the core rules engine that generates a baseline service plan from the profile. Based on the previous spec, the Service Intensity Matrix uses a hierarchy of evidence (legislative rules, CAP guidelines, research-based algorithms, best practices)  to decide what services and how much are minimally needed for safety. We will reimplement this matrix logic.

Design:
	•	Use the CAP trigger configs mentioned above: for each CAP triggered in the profile, add or adjust services. For example, Falls CAP triggers a baseline of PT and OT visits ; Pressure Ulcer CAP triggers wound care nursing and possibly an air mattress via OT , etc.
	•	Use other algorithms like RUG-III case mix to adjust intensity . The previous logic used RUG groups as a “guardrail” for how much service to allocate  – we will incorporate that so the bundle intensity is statistically grounded.
	•	Combine all relevant rules to produce a preliminary list of service items (each with a type, frequency/duration, and rationale).

Improvements: Our new engine will be more transparent and testable. We can implement it such that it outputs not just the service recommendations but also an audit trail of why each service was included (e.g., “Included 2 extra PSW visits because ADL score=5 (Extensive assistance) and patient lives alone” or “Triggered Lifeline because Falls CAP and lives alone”). This was partially done in the old system where scenario rationales showed algorithm scores . We’ll expand on that to ensure every rule applied can be traced. This not only helps with debugging and validation but also feeds nicely into the explanation feature (we can use these rationales for the AI to explain the bundle).

We will avoid mixing the old rules engine code by coding this from scratch, likely in a clear rule evaluation sequence or using a rules engine library if convenient. The benefit is removing any accumulated complexity from the prior iterative development, instead implementing cleanly according to the documented rules.

4. Scenario Generation (“Persona” Bundles)

Once the baseline is determined, the system generates multiple scenarios emphasizing different care philosophies (e.g., Rehab-focused, Safety-focused, Caregiver Relief)  . We will implement a ScenarioGenerator that takes the baseline services and then spins out variations by tuning certain parameters:
	•	Recovery-Focused Scenario (Rehab Bundle): Increase rehab services (PT/OT) to maximum for short-term improvement  , and limit personal support to encourage patient independence.
	•	Safety & Stability Scenario (Risk Management Bundle): Emphasize services that reduce risk (add electronic monitoring, more frequent check-ins, etc.)  .
	•	Caregiver Relief Scenario (Sustainability Bundle): Increase respite and any service that gives the informal caregiver breaks .

We will use the logic described in the prior documentation as a basis, but implement the generator in a generic way so that adding/removing scenario types is easy. For instance, we might have a configuration file that defines each scenario type with its rules (like a small script: “if scenario == Rehab: scale therapy x1.5, cap PSW hours at baseline, etc.”). But since scenario logic might be quite custom, a bit of hard-coding in code is acceptable, provided it’s well-documented.

Improvements: The new scenario generation will ensure that all scenarios still meet the baseline safety requirements (no scenario will remove a service that was deemed essential, it can only add or re-balance). This was likely a feature of the old one too, but we will enforce it. Additionally, we might generate a cost estimate for each scenario and include it in the output, which would be new (e.g., “Weekly cost ~$5,000 in scenario A vs $4,500 in scenario B” for comparison, if budget is a concern). We also plan to incorporate any new scenario ideas if relevant – for example, if policy shifts or user feedback suggests a scenario focused on Technology-Enabled Care, we could add that in future. The architecture will allow for it.

5. AI Explanation Service

This is the component that provides a human-readable explanation for a given bundle scenario – essentially translating the bundle and the rationale into a narrative (“We recommend this service mix because…”). The previous system used a BundleExplanationService that would call Vertex AI with a prompt, and had a rules-based fallback if AI was unavailable. In our new build, since we have local access to Claude (or we can call our own AI models), we will integrate this more tightly.

Design:
	•	We will create a prompt template (similar to the prior BundleExplanationPromptBuilder) that takes as input: the patient profile summary, the list of services in the bundle (with rationales), and possibly the scenario philosophy, then asks the AI to produce a concise explanation in layman’s terms.
	•	Because we want to run this within our environment, we might directly use the Claude API (via an SDK or via Cursor’s agent if that can be invoked programmatically) to get the explanation. This removes dependency on Google’s Vertex AI and allows more flexibility (Claude can use the full context of the profile if needed, given the large window).
	•	As a fallback, we will still implement a simple deterministic explanation generator (like the old rules-based fallback). This would produce a template-driven explanation if the AI call fails or if PII concerns prevent sending data. But since we can host the model ourselves, PII is less of an issue (we still strip names or identifiers in the prompt though, just use “the patient” etc., maintaining privacy).

Improvements: By using our on-hand AI model for explanations, we can iterate and refine the prompt more quickly (no external API latency or cost constraints beyond what we’ve already budgeted with our own model usage). We can also possibly generate explanations for multiple scenarios in parallel (using parallel agents) if needed. We will ensure the explanation text is consistent and accurate by testing it with various scenario outputs. If certain clinical terms appear, we might build a small glossary or ensure the AI defines them (for instance, if it says “PSW” we may want it to output “Personal Support Worker (PSW)” for clarity). These little touches will improve user understanding.

Another improvement is the possibility of interactive Q&A or refinement: since we have Claude accessible, we could allow the end user (or developer) to ask “Why was service X included?” and have the AI answer based on context. This is beyond the original scope but something to keep in mind for future features. Our new architecture should be able to support such extensions given its AI integration.

6. User Interface (UI)

The UI in the previous phase (Phase 7) provided things like a panel of algorithm scores, an “Explain” button for each scenario, and risk indicators. For the new build, we have two approaches:
	•	Reuse the Existing UI (if it was a separate web app) by adjusting it to call the new backend API. This would mean minimal changes on the front-end, focusing our effort on the backend rebuild. We might choose this if the UI tech stack remains suitable (e.g., it’s a React app that we can easily point to a new API endpoint). This aligns somewhat with Option B thinking (modify/adapt rather than rewrite everything).
	•	Rebuild the UI from scratch to better align with the new backend (Option C approach). If the old UI was heavily tied to old backend data structures or if we simply want a fresh design, we can reimplement it. Given time constraints, a full UI rewrite is secondary to backend functionality; however, since we want no mismatches between old/new (and to avoid “frankenstein” integration issues), we will likely modernize the UI code as well. Perhaps we use a modern framework (if not already used) and ensure the UI’s logic for presenting scenarios is simple and just consumes the new API.

We will aim for UI feature parity: the scenario cards, the explanation modal, etc., will all be present. One improvement could be to add more interactivity – e.g., allow the user to toggle certain assumptions (like “what if the caregiver could provide more hours?” and regenerate scenarios) – but that might be future work beyond this immediate rebuild.

For now, we’ll design the backend such that the API endpoints are well-defined (e.g., POST /v2/bundle-engine/generate to submit an assessment and get bundles, POST /v2/bundle-engine/explain to get explanation text as before). The UI can then be adjusted or rebuilt to hit these endpoints. Ensuring consistent API contracts (same or improved schema) means we don’t break any existing consumer of the API, if there is one.

7. Integration and Quality Assurance

Finally, once all components are ready, we will perform thorough testing:
	•	Unit tests for each module (we can generate many of these using the AI as well, asking it to produce test cases given a function’s specification).
	•	Integration tests that simulate a full input (a sample patient assessment) through to output (bundle scenarios with explanations) . We have examples from real use (like known high-intensity cases or typical patient profiles) we can use to validate results. The output should make sense clinically and match expectations. Any discrepancy (e.g., if new logic suggests something unreasonable) will be reviewed and corrected.
	•	Performance test: Because we are using AI at runtime for explanations (and possibly could for other things), we need to ensure the system responds in a reasonable time. Claude 4.5 is fast for text generation, but if a scenario explanation prompt is very large, it could take a bit. We might measure how long it takes and consider optimizations (like trimming the prompt). If needed, we can keep the explanation generation asynchronous (so the UI could display the bundle first and the explanation a few seconds later when ready).
	•	Peer Review: We might involve another developer or domain expert to review the new build (especially the clinical logic) to make sure we didn’t miss any important rule from the old system. Having everything rebuilt new means we must be careful not to drop any functionality that wasn’t explicitly in specs but was in old code. We’ll use the old feature list   as a checklist to verify each feature is accounted for in the new implementation.

Through these steps, we will achieve a comprehensive rebuild that is clean, coherent, and primed for future enhancements. All components will work together in the new codebase from day one, avoiding the pitfalls of mixing legacy components with new ones.

Key Result: At the end of this process, we expect to have a fully functional Home Care Bundle Builder system that can take an interRAI assessment and output several tailored care bundle scenarios, each with rationale and explanations, delivered via a modern API and UI. The system will be built using AI assistance but thoroughly verified, yielding a maintainable codebase. This positions us well for any subsequent additions (new scenarios, new assessment types, scaling up to production, etc.).

Appendix: Implementation Notes and Context Considerations

<!-- The following section provides additional notes and commentary on the implementation approach, focusing on context window management, tool usage, and mitigation of potential issues. These notes are intended for developers and team members to keep in mind throughout the build process. -->


	•	Managing Large Contexts in Prompts: Even though Claude 4.5 can handle very large inputs, it’s wise to keep prompts as concise as possible. Avoid repetition in the prompt (don’t supply the same block of text or spec twice). If multiple modules need the same context (e.g., a data model definition), consider defining it in one place and referencing it in prompts (Cursor allows an agent to read files from the repo; the agent could open the file with class definitions to incorporate that content instead of us manually pasting it). This keeps prompts cleaner and reduces token usage.
	•	Parallel Development Synchronization: When using parallel agents, ensure there is a short synchronization step before final integration. If Agent1 and Agent2 both define overlapping pieces (like two agents define the same class differently), we may need to reconcile. A strategy is to designate one agent as the source of truth for certain shared components or to stub interfaces first. For example, we could first run a quick agent task to generate just the interfaces (abstract classes or method signatures), and share those with all agents. This way, each parallel agent knows the contracts and will implement accordingly. Coordination upfront saves merge pain later.
	•	Cursor Agent Weaknesses and Mitigations: As noted by some developers, AI agents (including Cursor’s) can struggle as projects grow in complexity . They might introduce errors or inconsistencies if they lose track of context. To mitigate this:
	•	Keep conversations with a given agent focused. If an agent has done 10 turns and now the prompt history is huge, consider starting a fresh session for a new task (while feeding in the necessary context anew). Fresh prompts avoid the issue of the model getting confused by too much backscroll.
	•	Use the agent’s ability to run tests frequently. This grounds the AI in reality – an error message from a failed run is concrete feedback that it can act on, rather than it hallucinating whether the code is correct.
	•	If the agent output starts degrading (nonsensical or repetitive code), it may be hitting some limit or confusion. In such cases, halting and summarizing progress, then restarting with a clearer prompt (perhaps using a different model for that step) can help.
	•	We will also take advantage of Claude’s “extended thinking” and “chain-of-thought preservation” features   if available. Claude Sonnet 4.5, in particular, is noted for better long-horizon planning (working on tasks incrementally and keeping track of progress) . If we find Opus struggling with planning, we might try Sonnet 4.5 for those tasks (depending on access) as it’s optimized for complex agent behavior.
	•	Token Usage and Cost: With the Max plan and 20× quota, we have flexibility, but we should still be mindful. Running many parallel agents or very large prompts can consume a lot of tokens. We’ll monitor usage. If we find ourselves approaching limits, we can downscale by using medium effort or smaller context. Also, for any purely mechanical code (boilerplate getters/setters, etc.), we might use smaller models or even codegen tools rather than always invoking the expensive model.
	•	Quality Assurance via Multiple Models: Leveraging GPT-4 and Gemini is not just for initial coding; we can use them in later QA. For example, after everything is built, we might have GPT-4 do a final code review of the repository. It could potentially spot subtle issues (like a potential null pointer, or an off-by-one in an index) that our tests didn’t catch. This can be done by feeding critical sections or using automated linters augmented by AI.
	•	Documentation and Comments: We will instruct the AI to include clear comments in the code, but we’ll also review them. Sometimes AI comments might be incorrect if the code evolved; we’ll correct any such instances. Additionally, we maintain this living spec document – it can be updated as we implement to reflect any changes. This spec itself can serve as documentation for future developers, so we might even include it (or an abridged version) in the repository for reference.
	•	Future Context Window Use at Runtime: While our immediate focus is building the system, looking ahead, we might harness Claude’s large context at runtime for advanced features (like reasoning over a patient’s entire history, which could be lengthy). The architecture we build now should be open to that – for instance, our Explanation Service could pass a lot of info to the model if needed. We should structure prompts in a way that they degrade gracefully if truncated (maybe always put the most critical info first, less critical details last).
	•	“No Frankenstein” Principle: We reiterate that nothing from the old system is blindly plugged into the new without refactoring. If there is some legacy code or script that seems tempting to reuse, we will take the time to re-write it in the new style. Mixing codebases often leads to hidden bugs and maintenance nightmares, hence our decision for a clean break (Option C). The only things we carry over are knowledge and data: e.g., lists of services, algorithm thresholds, or domain insights gleaned from the old project. All code will be native to the new project.

By keeping these notes in mind, the development team can navigate the complexity of using cutting-edge AI tools effectively. This appendix can be seen as a set of “meta-guidelines” to ensure that the process of building the Home Care Bundle Builder is itself robust and efficient. We have the best tools at our disposal – now it’s about using them wisely to achieve a successful implementation.